I need you to debug an Azure AKS LoadBalancer connectivity issue as a senior software engineer.

PROBLEM: External LoadBalancer IP (4.156.121.111) times out on HTTP/HTTPS despite all components being healthy.

WHAT'S WORKING:
- All pods healthy (3/3 running)
- Internal connectivity perfect (NodePort 32662 returns full frontend HTML)
- Health checks passing (serviceProxyHealthy: true)
- LoadBalancer backend ports manually fixed (80→32662, 443→30699)
- NSG rules created for my IP (24.11.0.176/32) on ports 80, 443, 32662, 30699, 32600
- Backend pool has node registered
- Public IP pingable (70ms response)
- No Azure Firewall, App Gateway, or route tables blocking

WHAT'S NOT WORKING:
- curl http://4.156.121.111 → Connection timeout after 10 seconds
- telnet 4.156.121.111 80 → Connection timeout
- Even with NSG completely open (allow all), still fails

ALREADY DEBUGGED:
1. Fixed Azure CCM bug (backend ports were 80, now correctly 32662)
2. Created NSG rules for all required ports
3. Tested with NSG fully open (priority 101, allow all) - still failed
4. Removed loadBalancerSourceRanges restriction
5. Verified externalTrafficPolicy: Local
6. Verified iptables rules exist for LoadBalancer
7. Confirmed no Azure Firewall or Application Gateway
8. Tested internal connectivity (works perfectly)

ASSESSMENT: This appears to be an Azure platform-level networking issue, NOT a configuration problem. Even with NSG completely open, TCP traffic on ports 80/443 doesn't reach the pods, while ICMP (ping) works fine.

ENVIRONMENT:
- Cluster: mini-xdr-aks (East US)
- Resource Group: MC_mini-xdr-prod-rg_mini-xdr-aks_eastus
- Namespace: mini-xdr
- Service: mini-xdr-loadbalancer (type: LoadBalancer)
- NodePorts: HTTP=32662, HTTPS=30699, Health=32600
- My IP to whitelist: 24.11.0.176

WHAT I NEED:
1. Systematically debug the network path from internet → LoadBalancer → NodePort → Pod
2. Check for Azure subscription policies or restrictions
3. Investigate if Standard SKU LoadBalancer has limitations we're missing
4. Consider deploying NGINX Ingress Controller as alternative
5. Use Azure Network Watcher if needed for packet captures

TEST COMMANDS:
```bash
# External (fails):
curl -I http://4.156.121.111

# Internal (works):
kubectl exec -n mini-xdr deployment/mini-xdr-backend -- curl -s http://10.0.4.4:32662 | head -20

# Port forward (workaround):
kubectl port-forward -n mini-xdr svc/mini-xdr-frontend-service 3000:3000

# View config:
kubectl get svc mini-xdr-loadbalancer -n mini-xdr
az network lb rule list --resource-group MC_mini-xdr-prod-rg_mini-xdr-aks_eastus --lb-name kubernetes --output table
az network nsg rule list --resource-group MC_mini-xdr-prod-rg_mini-xdr-aks_eastus --nsg-name aks-agentpool-10857568-nsg --output table
```

START DEBUGGING: Think like a senior engineer. What Azure platform-level issue could cause this behavior where ping works but HTTP/HTTPS don't, even with NSG fully open?

