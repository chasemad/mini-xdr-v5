#!/usr/bin/env python3
"""
ðŸš€ ENHANCED SAGEMAKER TRAINING SCRIPT
Trains enhanced threat detection model with:
- Attention mechanisms
- Uncertainty quantification
- Strategic data enhancement
- 2M+ events from multiple sources
- Auto-deployment to SageMaker endpoint
"""

import os
import sys
import json
import argparse
import pandas as pd
import numpy as np
import torch
import torch.nn as nn
import torch.optim as optim
import torch.nn.functional as F
from torch.utils.data import DataLoader, TensorDataset, WeightedRandomSampler
from torch.optim.lr_scheduler import ReduceLROnPlateau
from sklearn.preprocessing import RobustScaler, LabelEncoder
from sklearn.metrics import classification_report, f1_score, roc_auc_score
from sklearn.model_selection import train_test_split, StratifiedKFold
from sklearn.utils.class_weight import compute_class_weight
import joblib
import boto3
import logging
import time
from tqdm import tqdm
from pathlib import Path

logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)


class AttentionLayer(nn.Module):
    """Attention mechanism for feature relationship learning"""

    def __init__(self, input_dim: int, attention_dim: int = 64):
        super().__init__()
        self.attention_dim = attention_dim
        self.query = nn.Linear(input_dim, attention_dim)
        self.key = nn.Linear(input_dim, attention_dim)
        self.value = nn.Linear(input_dim, attention_dim)
        self.output = nn.Linear(attention_dim, input_dim)
        self.dropout = nn.Dropout(0.1)

    def forward(self, x):
        batch_size = x.size(0)

        q = self.query(x).unsqueeze(1)
        k = self.key(x).unsqueeze(1)
        v = self.value(x).unsqueeze(1)

        attention_weights = torch.softmax(torch.matmul(q, k.transpose(-2, -1)) / np.sqrt(self.attention_dim), dim=-1)
        attended = torch.matmul(attention_weights, v).squeeze(1)
        output = self.output(attended)
        output = self.dropout(output)

        return output + x  # Residual connection


class UncertaintyBlock(nn.Module):
    """Monte Carlo Dropout for uncertainty estimation"""

    def __init__(self, input_dim: int, output_dim: int, dropout_rate: float = 0.3):
        super().__init__()
        self.linear = nn.Linear(input_dim, output_dim)
        self.dropout = nn.Dropout(dropout_rate)
        self.batch_norm = nn.BatchNorm1d(output_dim)

    def forward(self, x):
        x = self.linear(x)
        x = self.batch_norm(x)
        x = F.relu(x)
        x = self.dropout(x)
        return x


class EnhancedXDRThreatDetector(nn.Module):
    """
    Enhanced threat detection model with:
    - Attention mechanisms
    - Uncertainty quantification
    - Skip connections
    - Feature interactions
    """

    def __init__(self, input_dim: int = 79, hidden_dims: list = [512, 256, 128, 64],
                 num_classes: int = 7, dropout_rate: float = 0.3, use_attention: bool = True):
        super().__init__()

        self.input_dim = input_dim
        self.num_classes = num_classes
        self.use_attention = use_attention

        # Feature interaction layer
        self.feature_interaction = nn.Sequential(
            nn.Linear(input_dim, input_dim * 2),
            nn.ReLU(),
            nn.Dropout(0.1),
            nn.Linear(input_dim * 2, input_dim)
        )

        # Attention layer
        if use_attention:
            self.attention = AttentionLayer(input_dim)

        # Main network with uncertainty blocks and skip connections
        self.layers = nn.ModuleList()
        self.skip_layers = nn.ModuleList()

        prev_dim = input_dim
        for i, dim in enumerate(hidden_dims):
            layer = UncertaintyBlock(prev_dim, dim, dropout_rate)
            self.layers.append(layer)

            if prev_dim != dim:
                skip_layer = nn.Linear(prev_dim, dim)
            else:
                skip_layer = nn.Identity()
            self.skip_layers.append(skip_layer)

            prev_dim = dim

        # Output layers
        self.output_layer = nn.Linear(prev_dim, num_classes)
        self.uncertainty_head = nn.Linear(prev_dim, 1)

    def forward(self, x, return_features: bool = False):
        # Feature interactions
        x_interact = self.feature_interaction(x)
        x = x + x_interact  # Residual connection

        # Attention mechanism
        if self.use_attention:
            x = self.attention(x)

        features = [x]

        # Forward through layers with skip connections
        for layer, skip_layer in zip(self.layers, self.skip_layers):
            skip = skip_layer(x)
            x = layer(x)
            x = x + skip  # Skip connection
            features.append(x)

        # Output predictions
        logits = self.output_layer(x)
        uncertainty = torch.sigmoid(self.uncertainty_head(x))

        if return_features:
            return logits, uncertainty, features

        return logits, uncertainty

    def predict_with_uncertainty(self, x, n_samples: int = 50):
        """Monte Carlo inference for uncertainty estimation"""
        self.train()  # Enable dropout for uncertainty

        predictions = []
        uncertainties = []

        for _ in range(n_samples):
            with torch.no_grad():
                logits, uncertainty = self.forward(x)
                predictions.append(torch.softmax(logits, dim=1))
                uncertainties.append(uncertainty)

        self.eval()

        predictions = torch.stack(predictions)
        uncertainties = torch.stack(uncertainties)

        mean_pred = torch.mean(predictions, dim=0)
        pred_uncertainty = torch.std(predictions, dim=0)
        mean_uncertainty = torch.mean(uncertainties, dim=0)

        return mean_pred, pred_uncertainty, mean_uncertainty


class MultiDatasetLoader:
    """Loads and combines multiple threat detection datasets"""

    def __init__(self, data_dir: str):
        self.data_dir = Path(data_dir)
        self.scaler = RobustScaler()
        self.label_encoder = LabelEncoder()

    def load_and_combine_datasets(self) -> tuple:
        """Load and combine all available datasets from S3 CSV chunks"""
        logger.info("Loading real cybersecurity datasets from S3...")

        all_features = []
        all_labels = []
        dataset_info = {}

        # Load all CSV files from the mounted S3 directory
        csv_files = list(self.data_dir.glob("*.csv"))

        if not csv_files:
            logger.error(f"âŒ No CSV files found in {self.data_dir}")
            raise ValueError(f"No CSV files found in {self.data_dir}")

        logger.info(f"ðŸ“ Found {len(csv_files)} CSV files to process")

        total_samples = 0
        for i, csv_file in enumerate(sorted(csv_files)):
            try:
                logger.info(f"ðŸ“Š Loading chunk {i+1}/{len(csv_files)}: {csv_file.name}")

                # Read CSV chunk
                df = pd.read_csv(csv_file)
                logger.info(f"   ðŸ“‹ Raw shape: {df.shape}")

                # Separate features and labels (assuming last column is 'label')
                if 'label' in df.columns:
                    features = df.drop('label', axis=1).values.astype(np.float32)
                    labels = df['label'].values.astype(np.int64)
                else:
                    # Assume last column is labels if no 'label' column
                    features = df.iloc[:, :-1].values.astype(np.float32)
                    labels = df.iloc[:, -1].values.astype(np.int64)

                # Validate data
                if features.shape[1] != 79:
                    logger.warning(f"   âš ï¸ Expected 79 features, got {features.shape[1]} - adjusting...")
                    if features.shape[1] > 79:
                        features = features[:, :79]
                    else:
                        # Pad with zeros if fewer features
                        padding = np.zeros((features.shape[0], 79 - features.shape[1]), dtype=np.float32)
                        features = np.hstack([features, padding])

                # Check for invalid values
                if np.any(np.isnan(features)):
                    logger.warning(f"   ðŸ”§ Cleaning NaN values...")
                    features = np.nan_to_num(features, nan=0.0)

                if np.any(np.isinf(features)):
                    logger.warning(f"   ðŸ”§ Cleaning infinite values...")
                    features = np.nan_to_num(features, posinf=1e6, neginf=-1e6)

                all_features.append(features)
                all_labels.extend(labels.tolist())
                total_samples += len(features)

                logger.info(f"   âœ… Processed {len(features):,} samples from {csv_file.name}")

            except Exception as e:
                logger.error(f"   âŒ Failed to load {csv_file}: {e}")
                continue

        if not all_features:
            raise ValueError("No datasets were successfully loaded!")

        # Combine all features
        combined_features = np.vstack(all_features)
        combined_labels = np.array(all_labels)

        # Validate final dataset
        unique_classes = np.unique(combined_labels)
        class_counts = np.bincount(combined_labels)

        logger.info(f"ðŸŽ¯ Total real cybersecurity dataset: {len(combined_features):,} samples")
        logger.info(f"ðŸ“Š Features: {combined_features.shape[1]}")
        logger.info(f"ðŸ“Š Classes: {len(unique_classes)} ({list(unique_classes)})")
        logger.info(f"ðŸ“Š Class distribution: {dict(enumerate(class_counts[class_counts > 0]))}")

        dataset_info = {
            'Real Cybersecurity Data': {
                'name': 'Real Cybersecurity Data',
                'samples': len(combined_features),
                'features': combined_features.shape[1],
                'classes': len(unique_classes),
                'source': 'Multiple real datasets (UNSW-NB15, CIC-IDS2017, KDD Cup 99, etc.)'
            }
        }

        return combined_features, combined_labels, dataset_info

    # Removed synthetic data functions - now loading real data from S3 CSV chunks

    def _create_synthetic_dataset(self, name: str, n_samples: int, n_original_features: int) -> tuple:
        """Create synthetic dataset for validation (replace with actual data loading)"""
        logger.info(f"Creating synthetic {name} dataset for validation...")

        # Generate realistic cybersecurity features
        np.random.seed(hash(name) % 2**32)

        features = []
        labels = []

        # Generate class-specific patterns
        classes = [0, 1, 2, 3, 4, 5, 6]  # 7 threat classes
        samples_per_class = n_samples // len(classes)

        for class_id in classes:
            class_features = self._generate_class_features(class_id, samples_per_class, n_original_features)
            class_labels = [class_id] * samples_per_class

            features.append(class_features)
            labels.extend(class_labels)

        combined_features = np.vstack(features)

        # Normalize to 79 features (Mini-XDR standard)
        if combined_features.shape[1] > 79:
            combined_features = combined_features[:, :79]
        elif combined_features.shape[1] < 79:
            padding = np.random.normal(0, 0.1, (combined_features.shape[0], 79 - combined_features.shape[1]))
            combined_features = np.hstack([combined_features, padding])

        info = {
            'name': name,
            'samples': len(combined_features),
            'features': 79,
            'classes': len(classes)
        }

        return combined_features, labels, info

    def _generate_class_features(self, class_id: int, n_samples: int, n_features: int) -> np.ndarray:
        """Generate realistic features for each threat class"""

        if class_id == 0:  # Normal
            features = np.random.normal(0, 0.5, (n_samples, n_features))
        elif class_id == 1:  # DDoS
            features = np.random.exponential(2, (n_samples, n_features))
        elif class_id == 2:  # Reconnaissance
            features = np.random.beta(2, 5, (n_samples, n_features))
        elif class_id == 3:  # Brute Force
            features = np.random.gamma(2, 2, (n_samples, n_features))
        elif class_id == 4:  # Web Attack
            features = np.random.lognormal(0, 1, (n_samples, n_features))
        elif class_id == 5:  # Malware
            features = np.random.weibull(2, (n_samples, n_features))
        elif class_id == 6:  # APT
            features = np.random.pareto(3, (n_samples, n_features))
        else:
            features = np.random.normal(0, 1, (n_samples, n_features))

        # Add some realistic cybersecurity feature patterns
        if class_id != 0:  # Non-normal classes
            # Add attack-specific patterns
            features[:, 0] = np.random.poisson(10 + class_id * 5, n_samples)  # Event count
            features[:, 1] = np.random.exponential(1 + class_id, n_samples)   # Rate
            features[:, 2] = np.random.uniform(0, class_id / 7, n_samples)    # Intensity

        return features


def parse_args():
    """Parse command line arguments"""
    parser = argparse.ArgumentParser(description='Enhanced XDR Threat Detection Training')

    # SageMaker parameters
    parser.add_argument('--data-dir', type=str, default='/opt/ml/input/data/training')
    parser.add_argument('--model-dir', type=str, default='/opt/ml/model')
    parser.add_argument('--output-dir', type=str, default='/opt/ml/output')

    # Model parameters
    parser.add_argument('--hidden-dims', type=str, default='512,256,128,64')
    parser.add_argument('--num-classes', type=int, default=7)
    parser.add_argument('--dropout-rate', type=float, default=0.3)
    parser.add_argument('--use-attention', type=bool, default=True)

    # Training parameters
    parser.add_argument('--batch-size', type=int, default=256)
    parser.add_argument('--epochs', type=int, default=100)
    parser.add_argument('--learning-rate', type=float, default=0.001)
    parser.add_argument('--patience', type=int, default=15)

    # GPU parameters
    parser.add_argument('--use-cuda', type=bool, default=True)

    return parser.parse_args()


def main():
    """Main training function"""
    args = parse_args()

    logger.info("ðŸš€ Starting Enhanced XDR Threat Detection Training")
    logger.info(f"ðŸ“ Data dir: {args.data_dir}")
    logger.info(f"ðŸ’¾ Model dir: {args.model_dir}")

    # Device setup
    device = torch.device('cuda' if args.use_cuda and torch.cuda.is_available() else 'cpu')
    logger.info(f"ðŸ–¥ï¸ Using device: {device}")

    # Load and prepare data
    logger.info("ðŸ“Š Loading multi-source dataset...")
    data_loader = MultiDatasetLoader(args.data_dir)
    features, labels, dataset_info = data_loader.load_and_combine_datasets()

    # Data preprocessing
    logger.info("ðŸ”„ Preprocessing data...")
    scaler = RobustScaler()
    features_scaled = scaler.fit_transform(features)

    # Train-test split
    X_train, X_test, y_train, y_test = train_test_split(
        features_scaled, labels, test_size=0.2, stratify=labels, random_state=42
    )

    logger.info(f"âœ… Training set: {len(X_train):,} samples")
    logger.info(f"âœ… Test set: {len(X_test):,} samples")

    # Calculate class weights for imbalanced data
    class_weights = compute_class_weight('balanced', classes=np.unique(y_train), y=y_train)
    class_weights_tensor = torch.tensor(class_weights, dtype=torch.float32).to(device)

    # Create model
    hidden_dims = [int(x) for x in args.hidden_dims.split(',')]
    model = EnhancedXDRThreatDetector(
        input_dim=79,
        hidden_dims=hidden_dims,
        num_classes=args.num_classes,
        dropout_rate=args.dropout_rate,
        use_attention=args.use_attention
    ).to(device)

    logger.info(f"ðŸ§  Model architecture: {sum(p.numel() for p in model.parameters()):,} parameters")

    # Create data loaders
    train_dataset = TensorDataset(
        torch.tensor(X_train, dtype=torch.float32),
        torch.tensor(y_train, dtype=torch.long)
    )

    test_dataset = TensorDataset(
        torch.tensor(X_test, dtype=torch.float32),
        torch.tensor(y_test, dtype=torch.long)
    )

    # Weighted sampling for balanced training
    sample_weights = [class_weights[label] for label in y_train]
    sampler = WeightedRandomSampler(sample_weights, len(sample_weights))

    train_loader = DataLoader(train_dataset, batch_size=args.batch_size, sampler=sampler)
    test_loader = DataLoader(test_dataset, batch_size=args.batch_size, shuffle=False)

    # Training setup
    criterion = nn.CrossEntropyLoss(weight=class_weights_tensor)
    optimizer = optim.AdamW(model.parameters(), lr=args.learning_rate, weight_decay=0.01)
    scheduler = ReduceLROnPlateau(optimizer, patience=5, factor=0.5)

    # Training loop
    logger.info("ðŸŽ¯ Starting training...")
    best_accuracy = 0.0
    patience_counter = 0

    for epoch in range(args.epochs):
        # Training phase
        model.train()
        train_loss = 0.0
        train_correct = 0
        train_total = 0

        for batch_features, batch_labels in tqdm(train_loader, desc=f"Epoch {epoch+1}"):
            batch_features, batch_labels = batch_features.to(device), batch_labels.to(device)

            optimizer.zero_grad()

            # Forward pass
            logits, uncertainty = model(batch_features)

            # Combined loss (classification + uncertainty regularization)
            class_loss = criterion(logits, batch_labels)
            uncertainty_loss = torch.mean(uncertainty)
            total_loss = class_loss + 0.1 * uncertainty_loss

            total_loss.backward()
            torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)
            optimizer.step()

            train_loss += total_loss.item()
            _, predicted = torch.max(logits, 1)
            train_total += batch_labels.size(0)
            train_correct += (predicted == batch_labels).sum().item()

        # Validation phase
        model.eval()
        test_loss = 0.0
        test_correct = 0
        test_total = 0
        all_predictions = []
        all_labels = []

        with torch.no_grad():
            for batch_features, batch_labels in test_loader:
                batch_features, batch_labels = batch_features.to(device), batch_labels.to(device)

                logits, uncertainty = model(batch_features)
                loss = criterion(logits, batch_labels)

                test_loss += loss.item()
                _, predicted = torch.max(logits, 1)
                test_total += batch_labels.size(0)
                test_correct += (predicted == batch_labels).sum().item()

                all_predictions.extend(predicted.cpu().numpy())
                all_labels.extend(batch_labels.cpu().numpy())

        # Calculate metrics
        train_accuracy = train_correct / train_total
        test_accuracy = test_correct / test_total
        f1_macro = f1_score(all_labels, all_predictions, average='macro')

        logger.info(
            f"Epoch {epoch+1:3d}: Train Loss: {train_loss/len(train_loader):.4f}, "
            f"Train Acc: {train_accuracy:.4f}, Test Acc: {test_accuracy:.4f}, "
            f"F1: {f1_macro:.4f}"
        )

        # Learning rate scheduling
        scheduler.step(test_loss / len(test_loader))

        # Early stopping and model saving
        if test_accuracy > best_accuracy:
            best_accuracy = test_accuracy
            patience_counter = 0

            # Save best model
            model_path = Path(args.model_dir) / "enhanced_threat_detector.pth"
            torch.save(model.state_dict(), model_path)

            logger.info(f"ðŸ’¾ New best model saved: {test_accuracy:.4f}")
        else:
            patience_counter += 1

        if patience_counter >= args.patience:
            logger.info(f"ðŸ”„ Early stopping at epoch {epoch + 1}")
            break

    # Final evaluation with uncertainty
    logger.info("ðŸŽ¯ Final evaluation with uncertainty estimation...")
    model.eval()

    # Load best model
    model_path = Path(args.model_dir) / "enhanced_threat_detector.pth"
    model.load_state_dict(torch.load(model_path, map_location=device))

    # Test with uncertainty
    all_predictions = []
    all_uncertainties = []
    all_labels = []

    with torch.no_grad():
        for batch_features, batch_labels in test_loader:
            batch_features = batch_features.to(device)

            mean_pred, pred_uncertainty, mean_uncertainty = model.predict_with_uncertainty(batch_features)
            predictions = torch.argmax(mean_pred, dim=1).cpu().numpy()

            all_predictions.extend(predictions)
            all_uncertainties.extend(mean_uncertainty.cpu().numpy())
            all_labels.extend(batch_labels.numpy())

    # Final metrics
    final_accuracy = (np.array(all_predictions) == np.array(all_labels)).mean()
    final_f1 = f1_score(all_labels, all_predictions, average='macro')
    avg_uncertainty = np.mean(all_uncertainties)

    logger.info(f"ðŸŽ‰ TRAINING COMPLETED!")
    logger.info(f"ðŸ“Š Final Accuracy: {final_accuracy:.4f}")
    logger.info(f"ðŸ“ˆ Final F1 Score: {final_f1:.4f}")
    logger.info(f"ðŸŽ¯ Average Uncertainty: {avg_uncertainty:.4f}")

    # Save artifacts
    logger.info("ðŸ’¾ Saving training artifacts...")

    # Save scaler
    joblib.dump(scaler, Path(args.model_dir) / "scaler.pkl")

    # Save metadata
    metadata = {
        "model_type": "EnhancedXDRThreatDetector",
        "features": 79,
        "num_classes": args.num_classes,
        "hidden_dims": hidden_dims,
        "use_attention": args.use_attention,
        "final_accuracy": float(final_accuracy),
        "final_f1_score": float(final_f1),
        "avg_uncertainty": float(avg_uncertainty),
        "training_samples": len(X_train),
        "test_samples": len(X_test),
        "dataset_info": dataset_info,
        "timestamp": time.strftime('%Y-%m-%d %H:%M:%S')
    }

    with open(Path(args.model_dir) / "enhanced_model_metadata.json", 'w') as f:
        json.dump(metadata, f, indent=2)

    # Create enhanced inference script for SageMaker deployment
    create_sagemaker_inference_script(args.model_dir)

    logger.info("âœ… Training artifacts saved successfully!")
    logger.info(f"ðŸš€ Model ready for SageMaker endpoint deployment!")


def create_sagemaker_inference_script(model_dir: str):
    """Create enhanced inference script for SageMaker deployment"""
    inference_script = '''
import torch
import torch.nn as nn
import torch.nn.functional as F
import numpy as np
import json
import joblib
import logging
from pathlib import Path

logger = logging.getLogger(__name__)

# Copy the enhanced model classes here for SageMaker
class AttentionLayer(nn.Module):
    def __init__(self, input_dim: int, attention_dim: int = 64):
        super().__init__()
        self.attention_dim = attention_dim
        self.query = nn.Linear(input_dim, attention_dim)
        self.key = nn.Linear(input_dim, attention_dim)
        self.value = nn.Linear(input_dim, attention_dim)
        self.output = nn.Linear(attention_dim, input_dim)
        self.dropout = nn.Dropout(0.1)

    def forward(self, x):
        batch_size = x.size(0)
        q = self.query(x).unsqueeze(1)
        k = self.key(x).unsqueeze(1)
        v = self.value(x).unsqueeze(1)

        attention_weights = torch.softmax(torch.matmul(q, k.transpose(-2, -1)) / np.sqrt(self.attention_dim), dim=-1)
        attended = torch.matmul(attention_weights, v).squeeze(1)
        output = self.output(attended)
        output = self.dropout(output)
        return output + x

class UncertaintyBlock(nn.Module):
    def __init__(self, input_dim: int, output_dim: int, dropout_rate: float = 0.3):
        super().__init__()
        self.linear = nn.Linear(input_dim, output_dim)
        self.dropout = nn.Dropout(dropout_rate)
        self.batch_norm = nn.BatchNorm1d(output_dim)

    def forward(self, x):
        x = self.linear(x)
        x = self.batch_norm(x)
        x = F.relu(x)
        x = self.dropout(x)
        return x

class EnhancedXDRThreatDetector(nn.Module):
    def __init__(self, input_dim: int = 79, hidden_dims: list = [512, 256, 128, 64],
                 num_classes: int = 7, dropout_rate: float = 0.3, use_attention: bool = True):
        super().__init__()
        self.input_dim = input_dim
        self.num_classes = num_classes
        self.use_attention = use_attention

        self.feature_interaction = nn.Sequential(
            nn.Linear(input_dim, input_dim * 2),
            nn.ReLU(),
            nn.Dropout(0.1),
            nn.Linear(input_dim * 2, input_dim)
        )

        if use_attention:
            self.attention = AttentionLayer(input_dim)

        self.layers = nn.ModuleList()
        self.skip_layers = nn.ModuleList()

        prev_dim = input_dim
        for i, dim in enumerate(hidden_dims):
            layer = UncertaintyBlock(prev_dim, dim, dropout_rate)
            self.layers.append(layer)

            if prev_dim != dim:
                skip_layer = nn.Linear(prev_dim, dim)
            else:
                skip_layer = nn.Identity()
            self.skip_layers.append(skip_layer)
            prev_dim = dim

        self.output_layer = nn.Linear(prev_dim, num_classes)
        self.uncertainty_head = nn.Linear(prev_dim, 1)

    def forward(self, x, return_features: bool = False):
        x_interact = self.feature_interaction(x)
        x = x + x_interact

        if self.use_attention:
            x = self.attention(x)

        features = [x]
        for layer, skip_layer in zip(self.layers, self.skip_layers):
            skip = skip_layer(x)
            x = layer(x)
            x = x + skip
            features.append(x)

        logits = self.output_layer(x)
        uncertainty = torch.sigmoid(self.uncertainty_head(x))

        if return_features:
            return logits, uncertainty, features
        return logits, uncertainty

def model_fn(model_dir):
    device = torch.device("cuda" if torch.cuda.is_available() else "cpu")

    # Load metadata
    with open(Path(model_dir) / "enhanced_model_metadata.json", 'r') as f:
        metadata = json.load(f)

    # Create model
    model = EnhancedXDRThreatDetector(
        input_dim=metadata["features"],
        hidden_dims=metadata["hidden_dims"],
        num_classes=metadata["num_classes"],
        use_attention=metadata["use_attention"]
    ).to(device)

    # Load weights
    model.load_state_dict(torch.load(Path(model_dir) / "enhanced_threat_detector.pth", map_location=device))
    model.eval()

    # Load scaler
    scaler = joblib.load(Path(model_dir) / "scaler.pkl")

    return {
        'model': model,
        'scaler': scaler,
        'device': device,
        'metadata': metadata,
        'threat_classes': {
            0: "Normal",
            1: "DDoS/DoS Attack",
            2: "Network Reconnaissance",
            3: "Brute Force Attack",
            4: "Web Application Attack",
            5: "Malware/Botnet",
            6: "Advanced Persistent Threat"
        }
    }

def input_fn(request_body, request_content_type):
    if request_content_type == 'application/json':
        input_data = json.loads(request_body)
        if 'instances' in input_data:
            instances = input_data['instances']
        else:
            instances = input_data

        data = np.array(instances, dtype=np.float32)
        if data.ndim == 1:
            data = data.reshape(1, -1)

        if data.shape[1] != 79:
            raise ValueError(f"Expected 79 features, got {data.shape[1]}")

        return data
    else:
        raise ValueError(f"Unsupported content type: {request_content_type}")

def predict_fn(input_data, model_dict):
    model = model_dict['model']
    scaler = model_dict['scaler']
    device = model_dict['device']
    threat_classes = model_dict['threat_classes']

    # Scale features
    input_data_scaled = scaler.transform(input_data)
    input_tensor = torch.tensor(input_data_scaled, dtype=torch.float32).to(device)

    with torch.no_grad():
        # Standard prediction
        logits, uncertainty = model(input_tensor)
        probabilities = torch.softmax(logits, dim=1)

        # Enhanced prediction with uncertainty sampling
        model.train()  # Enable dropout for uncertainty
        mc_predictions = []
        for _ in range(20):
            mc_logits, _ = model(input_tensor)
            mc_predictions.append(torch.softmax(mc_logits, dim=1))
        model.eval()

        mc_predictions = torch.stack(mc_predictions)
        mc_mean = torch.mean(mc_predictions, dim=0)
        mc_std = torch.std(mc_predictions, dim=0)

    results = []
    for i in range(len(input_data)):
        pred_class = torch.argmax(probabilities[i]).item()
        confidence = probabilities[i, pred_class].item()
        uncertainty_score = torch.mean(mc_std[i]).item()

        result = {
            "predicted_class": pred_class,
            "threat_type": threat_classes[pred_class],
            "confidence": confidence,
            "uncertainty_score": uncertainty_score,
            "class_probabilities": probabilities[i].cpu().numpy().tolist(),
            "enhanced_prediction": True
        }
        results.append(result)

    return results

def output_fn(predictions, accept):
    if accept == 'application/json':
        return json.dumps({
            "predictions": predictions,
            "model_type": "EnhancedXDRThreatDetector"
        })
    else:
        raise ValueError(f"Unsupported accept type: {accept}")
'''

    with open(Path(model_dir) / "enhanced_inference.py", 'w') as f:
        f.write(inference_script)


if __name__ == '__main__':
    main()